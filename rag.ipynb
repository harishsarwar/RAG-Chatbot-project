{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"]=api_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct working on Langcahin Huggingface Endpoint\n",
    "* woring speed is so fast.\n",
    "* instead of downloading LLM model from HUGGINGFACE.Used them through api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"microsoft/Phi-3.5-mini-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking LLM model runs well or not.\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,\n",
    "                        max_new_tokens=100, \n",
    "                        temperature=0.7, \n",
    "                        huggingfacehub_api_token=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='microsoft/Phi-3.5-mini-instruct', huggingfacehub_api_token='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', max_new_tokens=100, temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='microsoft/Phi-3.5-mini-instruct', client=<InferenceClient(model='microsoft/Phi-3.5-mini-instruct', timeout=120)>, async_client=<InferenceClient(model='microsoft/Phi-3.5-mini-instruct', timeout=120)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for set for LLM model\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" and how does it work?\\n\\nMachine learning is a subset of artificial intelligence (AI) that enables systems to learn and improve from experience without being explicitly programmed. It involves algorithms and statistical models that computer systems use to perform tasks by generalizing from data. Here's a simplified explanation of how machine learning works:\\n\\n1. **Data Collection**: The first step is to gather large amounts of data relevant to the problem you want to solve. This data can be labeled (\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for response from HUGGINGFACE LLM model.\n",
    "llm.invoke(\"what is machine learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (0.3.14)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (0.3.14)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (0.3.29)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.14->langchain_community) (0.3.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.14->langchain_community) (2.10.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.3.1)\n",
      "Collecting pypdf\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pypdf) (4.12.2)\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain_community\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading/Read pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 51\n",
      "Length of a page: 1428\n",
      "Content of a page: Generativ e AI course\n",
      "2\n",
      "what is Generative AI \n",
      "Generative AI refers to AI technologies that can create new content, ideas, or \n",
      "data that are coherent and plausible, often resembling human-generated \n",
      "outputs. It has a plethora of practical applications in different domains such as \n",
      "computer vision, natural language processing, and music generation.\n",
      "Roadmap for Generative AI \n",
      "1. Foundation in Machine Learning and Deep Learning\n",
      "Ensure proficiency in probability, statistics, linear algebra, and calculus.\n",
      "Gain hands-on experience with programming languages like Python/R.\n",
      "Familiarize yourself with supervised and unsupervised learning algorithms.\n",
      "Build machine learning models on tabular datasets.\n",
      "2. Deep Learning Mastery\n",
      "Develop a solid understanding of deep learning architectures such as \n",
      "MLPs, RNNs, LSTMs, GRUs, and CNNs.\n",
      "Gain proficiency in at least one deep learning framework like Keras, \n",
      "TensorFlow, PyTorch, or FastAPI.\n",
      "Train MLPs on tabular datasets.\n",
      "Construct RNNs and CNNs for unstructured data (text and image).\n",
      "3. Advanced Deep Learning Techniques\n",
      "Learn about pretrained models for image data and their types.\n",
      "Understand language models and build them using LSTMs/GRUs.\n",
      "Explore attention mechanisms and their applications.\n",
      "Study autoencoders and GANs architectures and train these models on \n",
      "datasets.\n",
      "4. Generative Models for NLP:\n",
      "Discover Large Language Models LLMs) like Transformers, BERT, GPT, \n",
      "PaLM, etc.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Generative AI course 0197850b485d44c8b762def06b893ee5.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting into Chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 70\n",
      "Length of a chunk: 1428\n",
      "Content of a chunk: Generativ e AI course\n",
      "2\n",
      "what is Generative AI \n",
      "Generative AI refers to AI technologies that can create new content, ideas, or \n",
      "data that are coherent and plausible, often resembling human-generated \n",
      "outputs. It has a plethora of practical applications in different domains such as \n",
      "computer vision, natural language processing, and music generation.\n",
      "Roadmap for Generative AI \n",
      "1. Foundation in Machine Learning and Deep Learning\n",
      "Ensure proficiency in probability, statistics, linear algebra, and calculus.\n",
      "Gain hands-on experience with programming languages like Python/R.\n",
      "Familiarize yourself with supervised and unsupervised learning algorithms.\n",
      "Build machine learning models on tabular datasets.\n",
      "2. Deep Learning Mastery\n",
      "Develop a solid understanding of deep learning architectures such as \n",
      "MLPs, RNNs, LSTMs, GRUs, and CNNs.\n",
      "Gain proficiency in at least one deep learning framework like Keras, \n",
      "TensorFlow, PyTorch, or FastAPI.\n",
      "Train MLPs on tabular datasets.\n",
      "Construct RNNs and CNNs for unstructured data (text and image).\n",
      "3. Advanced Deep Learning Techniques\n",
      "Learn about pretrained models for image data and their types.\n",
      "Understand language models and build them using LSTMs/GRUs.\n",
      "Explore attention mechanisms and their applications.\n",
      "Study autoencoders and GANs architectures and train these models on \n",
      "datasets.\n",
      "4. Generative Models for NLP:\n",
      "Discover Large Language Models LLMs) like Transformers, BERT, GPT, \n",
      "PaLM, etc.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(pages)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading Embedding model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdhar_hxm\\AppData\\Local\\Temp\\ipykernel_1372\\1771193664.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" # Embeddig model from hugging face we can used random embedding model\n",
    "                                                       # just bu calling HuggingFaceEmbeddings() modul\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04646666720509529,\n",
       " 0.006701764650642872,\n",
       " -0.012131502851843834,\n",
       " -0.021836593747138977,\n",
       " 0.052885752171278]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheching it is runing or not\n",
    "vector = embeddings.embed_query(\"hello, world\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vectors Storing\n",
    "* CromaDB is a vector database. where i stores embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_chroma\n",
      "  Using cached langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain_chroma)\n",
      "  Using cached chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi<1,>=0.95.2 (from langchain_chroma)\n",
      "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_chroma) (0.3.29)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_chroma) (1.26.4)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.10.4)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp39-cp39-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading posthog-3.7.5-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading onnxruntime-1.19.2-cp39-cp39-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading tokenizers-0.20.3-cp39-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached grpcio-1.68.1-cp39-cp39-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached bcrypt-4.2.1-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading mmh3-5.0.1-cp39-cp39-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.13)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain_chroma)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.5.0)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: requests in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain_chroma) (1.0.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached protobuf-5.29.2-cp39-cp39-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: sympy in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.13.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading wrapt-1.17.0-cp39-cp39-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.27.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.27.0)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading httptools-0.6.4-cp39-cp39-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading watchfiles-1.0.3-cp39-cp39-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading websockets-14.1-cp39-cp39-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from importlib-resources->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.21.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.12.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.4.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Using cached chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "Downloading chroma_hnswlib-0.7.6-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Using cached bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached grpcio-1.68.1-cp39-cp39-win_amd64.whl (4.4 MB)\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Downloading mmh3-5.0.1-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Downloading onnxruntime-1.19.2-cp39-cp39-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.2/11.1 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.1 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 9.1 MB/s eta 0:00:00\n",
      "Using cached opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Using cached opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.7.5-py2.py3-none-any.whl (54 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading tokenizers-0.20.3-cp39-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.8/2.4 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 9.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading httptools-0.6.4-cp39-cp39-win_amd64.whl (89 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached protobuf-5.29.2-cp39-cp39-win_amd64.whl (434 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading watchfiles-1.0.3-cp39-cp39-win_amd64.whl (284 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-14.1-cp39-cp39-win_amd64.whl (163 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.17.0-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53834 sha256=96cc79361d45ff46b9d35938518dc4e4aaacdb5d3bc409ddeba30a35dcc0799d\n",
      "  Stored in directory: c:\\users\\mdhar_hxm\\appdata\\local\\pip\\cache\\wheels\\f7\\02\\64\\d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, wrapt, websockets, websocket-client, tomli, shellingham, pyreadline3, pyproject_hooks, pyasn1, protobuf, overrides, opentelemetry-util-http, oauthlib, mmh3, mdurl, importlib-resources, httptools, grpcio, click, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, humanfriendly, googleapis-common-protos, deprecated, build, tokenizers, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, google-auth, fastapi, coloredlogs, typer, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain_chroma\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 cachetools-5.5.0 chroma-hnswlib-0.7.6 chromadb-0.5.23 click-8.1.8 coloredlogs-15.0.1 deprecated-1.2.15 durationpy-0.9 fastapi-0.115.6 flatbuffers-24.12.23 google-auth-2.37.0 googleapis-common-protos-1.66.0 grpcio-1.68.1 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-31.0.0 langchain_chroma-0.1.4 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.0.1 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.5 protobuf-5.29.2 pyasn1-0.6.1 pyasn1-modules-0.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rich-13.9.4 rsa-4.9 shellingham-1.5.4 starlette-0.41.3 tokenizers-0.20.3 tomli-2.2.1 typer-0.15.1 uvicorn-0.34.0 watchfiles-1.0.3 websocket-client-1.8.0 websockets-14.1 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\RAG-ChatBot\\rag-venv\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing proccess.\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Retrieving data from cromadb.\n",
    "* retrieve data from cromadb which is very simmiler to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "retriever_docs = retriever.invoke(\"about gen ai course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generativ e AI course\n",
      "1\n",
      "🪧\n",
      "Generative AI course \n",
      "\u0000What is Generative AI?\n",
      "\u0000Roadmap of Generative AI?\n",
      "\u0000What things do we need to learn in a Generative AI course?\n",
      "\u0000What are the foundational models in Generative AI?\n",
      "\u0000What is the future scope of Generative AI?\n",
      "\u0000What is the eligibility for a Generative AI course?\n",
      "\u0000Why should you learn Generative AI?\n",
      "\u0000What are the types of Generative AI compared to other AI types?\n",
      "\u0000What are the advantages and disadvantages of Generative AI?\n"
     ]
    }
   ],
   "source": [
    "print(retriever_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_huggingface) (0.27.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_huggingface) (0.3.29)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_huggingface) (3.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_huggingface) (0.20.3)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain_huggingface) (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.2.10)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.10.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
      "Collecting tokenizers>=0.19.1 (from langchain_huggingface)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.12.14)\n",
      "Requirement already satisfied: networkx in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\rag-chatbot\\rag-venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "Successfully installed tokenizers-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.LLM Model\n",
    "* apply LLM model on those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceEndpoint(repo_id=repo_id,\n",
    "                        max_new_tokens=100, \n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        repetition_penalty=1.1,\n",
    "                        return_full_text=True,\n",
    "                        huggingfacehub_api_token=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prompting\n",
    "* Use for better outcomes through question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Answer the question based on your knowledge. Use the following context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "</s>\n",
    "<|user|>\n",
    "{question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Chain\n",
    "* used for merged LLM model and prompt and all other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. final chain\n",
    "* passing parameters of prompt.\n",
    "* chain them out with llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough: A component in LangChain that passes data through without modification, useful for debugging or chaining processes.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <|system|>\n",
      "Answer the question based on your knowledge. Use the following context to help:\n",
      "\n",
      "[Document(metadata={'page': 31, 'source': 'Generative AI course 0197850b485d44c8b762def06b893ee5.pdf'}, page_content='Generativ e AI course\\n32\\npre training : \\nData Collection\\nGeneral Text Data\\nSpecialized Data\\nData Preprocessing\\nQuality Filtering\\nDeduplication\\nTokenization\\nHow Does Pretraining Affect LLMs?\\nMixture of Sources\\nAmount of Pretraining Data\\nQuality of Pretraining Data\\nArchitecture for Pretraining\\nEncoder Decoder Architecture\\nCausal Decoder Architecture\\nPrefix Decoder Architecture\\nEmergent Architectures\\nSome Notes on Configurations\\nLayer Normalization\\nAttention\\nPositional Encoding\\nPretraining Tasks\\nLanguage Modeling\\nDenoising Autoencoding\\nMixture-of-Denoisers'), Document(metadata={'page': 27, 'source': 'Generative AI course 0197850b485d44c8b762def06b893ee5.pdf'}, page_content=\"Generativ e AI course\\n28\\n1. Data Preparation: Data Preprocessing and Filtering\\nIn this stage, the focus is on preparing the data for training by performing \\npreprocessing and filtering. This involves:\\nCleaning and organizing the data to remove noise and inconsistencies.\\nPreprocessing text data by tokenizing, normalizing, and removing stop \\nwords or irrelevant information.\\nFiltering the data to ensure it meets quality standards and is relevant to the \\nmodel's objectives.\\n2. Train Model: Select a Foundational Model and Begin Training\\nHere, the foundational model is selected based on the task requirements, and \\ntraining is initiated by providing the model with tokenized data. Steps include:\\nChoosing an appropriate foundational model, considering factors like \\narchitecture, size, and pre-trained weights.\\nProviding tokenized input data to the model to start the training process.\\nMonitoring and adjusting training parameters to optimize model \\nperformance and convergence.\\n3. Validate the Model: Create Model Card\\nValidation involves creating a model card to provide comprehensive information \\nabout the trained model. Key components include:\")]\n",
      "\n",
      "\n",
      "<|user|>\n",
      "Pretraining Tasks\n",
      "\n",
      "<|assistant|>\n",
      "\n",
      " The document discusses various aspects related to \"Pretraining Tasks\" in Generative AI courses as part of understanding how these tasks affect Large Language Models (LLMs). Here are some key points extracted from that section along with their significance:\n",
      "\n",
      "1. **Language Modeling** - Involves feeding large volumes of raw language texts into models without any specific prompt so they learn patterns within natural languages such as grammar rules, sentence structures, common phrases etc\n"
     ]
    }
   ],
   "source": [
    "question = \"Pretraining Tasks\"\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# Making the response readable\n",
    "response = response.replace(\"</s>\", \"\").strip()\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
